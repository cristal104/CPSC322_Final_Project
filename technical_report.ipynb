{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical Report: Finding Fraudelent Activity  \n",
    "Project Name: Fraudulent or Not?  \n",
    "Team Members: Cristal Meza and Luke Nguyen  \n",
    "CPSC 322, Spring 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction  \n",
    "For this project, we decided upon a dataset that contained a number of transactions in which several were flagged as fraudelent and several were flagged as actually being fraudelent. We saw that this would make a good dataset for classification as we can train a classifier to be able to flag and determine from several attributes if a bank transaction is fraudelent or not.  \n",
    "The \"isFraud\" class label on the dataset is the one that was chosen to be y_train for our dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the CSV file to a table\n",
    "import myutils\n",
    "import plot_utils\n",
    "header, table = myutils.read_csv_to_table(\"Fraud.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Information about the dataset  \n",
    "Class labels (taken directly from kaggle):  \n",
    "* step - maps a unit of time in the real world. In this case 1 step is 1 hour of time. Total steps 744 (30 days simulation).  \n",
    "\n",
    "* type - CASH-IN, CASH-OUT, DEBIT, PAYMENT and TRANSFER.  \n",
    "\n",
    "* amount - amount of the transaction in local currency.  \n",
    "\n",
    "* nameOrig - customer who started the transaction  \n",
    "\n",
    "* oldbalanceOrg - initial balance before the transaction  \n",
    "\n",
    "* newbalanceOrig - new balance after the transaction  \n",
    "\n",
    "* nameDest - customer who is the recipient of the transaction  \n",
    "\n",
    "* oldbalanceDest - initial balance recipient before the transaction. Note that there is not information for customers that start with M (Merchants).  \n",
    "\n",
    "* newbalanceDest - new balance recipient after the transaction. Note that there is not information for customers that start with M (Merchants).  \n",
    "\n",
    "* isFraud - This is the transactions made by the fraudulent agents inside the simulation. In this specific dataset the fraudulent behavior of the agents aims to profit by taking control or customers accounts and try to empty the funds by transferring to another account and then cashing out of the system.  \n",
    "\n",
    "* isFlaggedFraud - The business model aims to control massive transfers from one account to another and flags illegal attempts. An illegal attempt in this dataset is an attempt to transfer more than 200.000 in a single transaction.  \n",
    "\n",
    "Class attributes:  \n",
    "\n",
    "* isFraud: We would try to predict if the particular transaction is fraudulennt and this class label is the answer,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevant Summary Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data visualizations  \n",
    "Below are some data visualizations that highlight important and unteresting aspects of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie Chart - Number of transactions flagged as fraudelent\n",
    "flagged_values, flagged_count = myutils.get_frequencies(table, header, \"isFlaggedFraud\")\n",
    "plot_utils.pie_chart(flagged_values, flagged_count, \"Figure 1 - Number of transactions flagged as fraudelent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie Chart - Number of transactions determined as fraudelent\n",
    "fraud_values, fraud_count = myutils.get_frequencies(table, header, \"isFraud\")\n",
    "plot_utils.pie_chart(fraud_values, fraud_count, \"Figure 2 - Number of transactions determiined as fraudelent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar Graph - Number of transaction types\n",
    "type_values, type_count =  myutils.get_frequencies(table, header, \"type\")\n",
    "plot_utils.bar_chart(type_values, type_count, \"Figure 3 - Number of types of transactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning  \n",
    "The CSV file did not have any missing data, but it was decided to remove a number of class attributes prior to classification. The removed attributes are:  \n",
    "* \"step\" - We did not deem this attribute as relevant to be included in X_train\n",
    "* \"isFlagged\" - We deemed this attribute as too similar to \"isFraud,\" which is the class label that is used for y_train.  \n",
    "\n",
    "Some attributes were changed for better classification and for the reader to be able to better understand the results.  \n",
    "* \"isFraud\" - The values were changed from \"0\" and \"1\" to \"no\" and \"yes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove \"step\" and \"isFlagged\" from the dataset\n",
    "myutils.drop_cols(table, header, \"step\")\n",
    "myutils.drop_cols(table, header, \"isFlaggedFraud\")\n",
    "\n",
    "# change attributes\n",
    "myutils.change_isFraud(table, header, \"isFraud\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import myclassifiers\n",
    "import myevaluation\n",
    "\n",
    "# create X_train and y_train \n",
    "y_actual = myutils.get_column(table, header, header[-1])\n",
    "X_train, X_test, y_train, y_test  = myevaluation.train_test_split(table,y_actual, random_state=20)\n",
    "myutils.drop_cols(table, header, \"isFraud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit data into Decision Tree\n",
    "print(\"Decision Tree\")\n",
    "tree_clf = myclassifiers.MyDecisionTreeClassifier()\n",
    "tree_clf.fit(X_train, y_train)\n",
    "y_predicted = tree_clf.predict(X_test)\n",
    "\n",
    "# results \n",
    "accuracy = myevaluation.accuracy_score(y_test, y_predicted, normalize=True)\n",
    "error_score = 1.0 - accuracy\n",
    "print(\"Error score:\", round(error_score, 2) * 100, \"%\")\n",
    "\n",
    "binary_score = myevaluation.binary_precision_score(y_test, y_predicted, labels=[\"no\", \"yes\"], pos_label=\"no\")\n",
    "print(\"Binary Precision Score:\", round(binary_score, 3))\n",
    "\n",
    "recall = myevaluation.binary_recall_score(y_test, y_predicted, labels=[\"no\", \"yes\"], pos_label=\"no\")\n",
    "print(\"Binary Recall Score:\", round(recall))\n",
    "\n",
    "f1 = myevaluation.binary_f1_score(y_test, y_predicted, labels=[\"no\", \"yes\"], pos_label=\"no\")\n",
    "print(\"Binary f1 score:\", round(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit data into Dummy classifier\n",
    "print(\"Dummy Classifier\")\n",
    "dummy_clf = myclassifiers.MyDummyClassifier()\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "y_predicted = dummy_clf.predict(X_test)\n",
    "\n",
    "# results \n",
    "accuracy = myevaluation.accuracy_score(y_test, y_predicted, normalize=True)\n",
    "error_score = 1.0 - accuracy\n",
    "print(\"Error score:\", error_score * 100, \"%\")\n",
    "\n",
    "binary_score = myevaluation.binary_precision_score(y_test, y_predicted, labels=[\"no\", \"yes\"], pos_label=\"no\")\n",
    "print(\"Binary Precision Score:\", round(binary_score, 3))\n",
    "\n",
    "recall = myevaluation.binary_recall_score(y_test, y_predicted, labels=[\"no\", \"yes\"], pos_label=\"no\")\n",
    "print(\"Binary Recall Score:\", round(recall))\n",
    "\n",
    "f1 = myevaluation.binary_f1_score(y_test, y_predicted, labels=[\"no\", \"yes\"], pos_label=\"no\")\n",
    "print(\"Binary f1 score:\", round(f1))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
